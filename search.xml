<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[ADS SSP DSP DMP全面解析]]></title>
    <url>%2F%E8%90%A5%E9%94%80%2FADS-SSP-DSP-DMP%E5%85%A8%E9%9D%A2%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[##一、概念 关系## 简单来讲，publishers是众多手足无措的小广告位、小朋友，ADN就是家长，去管理领取这些小朋友，让他们去学东西，长知识，但是呢，他们需要一个场所，学校，SSP就相当于一个学校，用来管理这些家长和小孩。这之后，就需要去选择要学习的东西了，学校怎么分配小孩和家长，这个交给教导主任DSP，他负责管理这些事情，小李是适合学习音乐，就让他去学音乐，家长掏钱，一个原理，在这之后呢，学校人数越来越多了，不方便管理与分配了，就需要一个教务系统了，用来统计学生与科目，以及对每个小孩进行分析，得出数据，教导主任得到这个数据之后，对每个小孩进行精细化教育分配，这就是这一套体系的前因后果。 Ad Network：由于碎片化的传播环境导致了网络上出现了N多的中小广告位（publishers），为增加这些publishers的价值，一种名为Ad Network的agency出现，负责整合这些publishers并与广告主谈判、交易。但由于：1、Ad Network自主制定交易价格导致publishers无利可图；2、提供的资源质量良莠不齐，流量不精准使广告主抱怨；3、再加上各Ad Network之间贩卖资源导致市场混乱； Ad Exchange：为解决以上问题便出现了Ad Exchange。它整合publishers及Ad Network，并统一称为“供应方”（supply side），通过“价高者得”的竞价机制对publishers进行购买并最终由技术支持实现“实时竞价”即RTB（Real Time Bidding），加上对购买过程及页面的优化，从而提升其竞争力。但由于：1、 平台上展示的publishers絮乱，而市场上又有多个Ad Exchange，广告主应该选择哪个Ad Exchange，怎样判断不同的publishers背后是否有target audience及应该如何出价，使广告主疑惑；2、 各Ad Exchange的储存量可是天文数字，广告主的购买过程将十分繁琐与复杂； DSP（Demand Side Platform）：有问题自然就有解决方法，一个协助广告主玩转Ad Exchange，名为DSP（Demand Side Platform）的需求方平台应运而生。DSP将Ad Exchange与自己驳接后向广告主提供一套更为简单的购买方式（决策及购买过程）。DSP深谙每一个广告背后都是一部分的受众的道理，对原来广告位的购买概念进行淡化，并提出target audience的购买概念。广告主只需要在DSP上告诉平台自己的目标人群及愿意出多少钱获取这些人群，平台就会通过后台强大的数据及运算方式帮你在Ad Exchange上操作，也就是进行程序化购买。而除了DSP与Ad Exchange之外，还有一个称为SSP（Supply Side Platform），即供应方平台的存在。SSP其实就是管理着publishers 和 Ad Networks这些供应渠道与Ad Exchange之间的供给关系。而程序化购买的要点就是“强大的数据及运算方式”。具体表现为：1、 海量及准确的受众数据，保证投放的精准性；2、 强大的自动化算法，保证最合理的竞价（RTB）；其中，受众的数据最为重要，很多DSP自己并没有受众数据又或者有但并不全面、准确。这时市场上又出现了另一个专业的提供者—DMP（Data Management Platform），数据管理平台。 DMP（Data Management Platform）：简单讲，它们手中握有受众数据，并能让 DSP 驳接到他们，利用它们所有的数据。使投放更为精准。而DSP为获取数据至少需要;1、 通过追踪，获取用户上网时产生的cookie，作为basic data；2、 实现跨域追踪，跨域追踪即在多个不同的域名中获取同一个用户所产生的数据，而跨域又包括跨主域及跨子域； 定义 在线广告的演化进程催生出程序化购买的概念，即把从广告主到媒体的全部投放过程程序化，通常需要一个程序化平台去购买广告展示。 程序化购买主要分“公开竞价”、“私有市场”两类交易方式， DSP-需求方平台也就是广告主服务平台，广告主可以通过DSP平台设置自己想要的受众目标以及愿意出多少钱购买这些受众的曝光等操作完成广告投放，面向广告购买方。 SSP-供应方平台它是媒体服务平台，媒体方可以通过SSP平台完成广告资源的管理，如流量分配、价格、筛选等等，面向广告售卖方。 ADX-广告交易平台连接买方和卖方，ADX将媒体的广告流量以拍卖的方式卖给DSP。 DMP-数据管理平台整合各方数据并提供数据分析，数据管理、数据调用等，用来指导广告主进行广告优化和投放决策。 如图所示： ##二、DSP广告系统架构及关键技术解析## 广告和网络游戏是互联网企业主要的盈利模式 广告是广告主通过媒体以尽可能低成本的方式与用户达成接触的商业行为。也就是说按照某种市场意图接触相应人群，影响其中潜在用户，使其选择广告主产品的几率增加，或对广告主品牌产生认同，通过长期的影响逐步形成用户对品牌的转化。 一个好的DSP系统需要满足： 拥有强大的RTB(Real-Time Bidding)的基础设施和能力。 拥有先进的用户定向(Audience Targeting)技术。 首先，DSP对其数据运算技术和速度要求非常之高。从普通用户在浏览器中地址栏输入网站的网址，到用户看到页面上的内容和广告这短短几百毫秒之内，就需要发生了好几个网络往返(Round Trip)的信息交换。 Ad Exchange首先要向DSP发竞价(bidding)请求，告知DSP这次曝光的属性，如物料的尺寸、广告位出现的URL和类别、以及用户的Cookie ID等；DSP接到竞价请求后，也必须在几十毫秒之内决定是否竞价这次曝光, 如果决定竞价，出什么样的价格，然后把竞价的响应发回到Ad Exchange。 如果Ad Exchange判定该DSP赢得了该次竞价，要在极短时间内把DSP所代表的广告主的广告迅速送到用户的浏览器上。整个过程如果速度稍慢，Ad Exchange就会认为DSP超时而不接受DSP的竞价响应，广告主的广告投放就无法实现。 其次，基于数据的用户定向(Audience Targeting)技术，则是DSP另一个重要的核心特征。从网络广告的实质上来说，广告主最终不是为了购买媒体，而是希望通过媒体与他们的潜在客户即目标人群进行广告沟通和投放。 服务于广告主或者广告主代理的DSP，则需要对Ad Exchange每一次传过来的曝光机会，根据关于这次曝光的相关数据来决定竞价策略。这些数据包括本次曝光所在网站、页面的信息，以及更为关键本次曝光的受众人群属性，人群定向的分析直接决定DSP的竞价策略。DSP在整个过程中，通过运用自己人群定向技术来分析，所得出的分析结果将直接影响广告主的广告投放效果。 此次分享主要针对以下几个方面，描述DSP广告系统架构及关键技术： 广告系统概念介绍 广告系统业务流程 DSP系统架构 RTB竞价引擎结构 点击率预测 DMP数据处理架构 受众定向划分 用户画像与广告系统反作弊 程序化购买的特点下图是在DSP产生之前和产生之后广告行业的两种最常见产业链 传统的广告投放模式的产业链是广告主通过广告代理，以广告网络/联盟为渠道在媒体网站展示广告，达到接触受众的目的的过程。 这种模式的好处是媒体网站可以通过通过包段或CPS的模式可以售出自己的广告位，但是这类售出是偏粗放型的，长期同类型的广告投放，受众会视觉疲劳，点击率会下降，转化也会随之下降。为了能够获得更多的收益，媒体必须通过差异化销售细分自己的广告位和受众。而事实上显示广告领域最初的定向投放的最初动机是供给方拆分流量以获得更高的营收。好的位置，通过包段通常会供不应求，但是对于长尾流量通常是会无人问津，即便是对于广告主来说同一个潜在客户在大媒体出现会有广告主包段进行购买，但是在小网站出现就会没人买。事实上潜在客户在哪里出现对于广告主都是同一个人，如果能显示与客户需求相吻合或接近的广告就有可能产生转化。在将优质广告位包段售出后，如果对用户有足够的认识，有足够多不同类型的广告主，在流量可以拆分到单次展现的购买粒度，就有可能依据不同的受众定向为每个广告主找到合适的人群和流量。 程序化购买颠覆了原有广告产业链，形成了全新的产业链。 鉴于群里有很多人不是做广告系统的，为了能够在后续的介绍过程中更容易理解介绍的内容，这里先介绍一些广告行业中常见的一些概念。 DSP（Demand Side Platform），是广告需求方平台，DSP为广告主提供跨媒介、跨平台、跨终端的的广告投放平台，通过数据整合、分析实现基于受众的精准投放，并且实时监控不断优化。 RTB（Real Time Bidding）实时竞价是DSP、广告交易平台等在网络广告投放中采用的主要售卖形式，会在极端的时间内（通常是50~100毫秒以内）通过对目标受众竞价的方式获得该次广告的展现，RTB的购买方式无论在PC端或是移动端均可以实现。 程序化购买（Programmatic Buying）根据广告主定义的期望受众，系统帮助其找出优选的媒体来购买受众，为广告主提出最优媒介采买计划，通过程序化购买的方式执行，并按照期望的周期反馈监测结果，并对后续投放进行优化。包括但不仅限于RTB购买。 最常见的DSP行业中的供需业务流，广告主作为需求方，潜在客户是最终的受众，中间穿插着代理机构，DSP，AdNetwork，AdExchange,SSP和供应方也就是媒体。 下图是DSP平台的广告投放流程，投放过程中涉及到广告受众，媒体网站，adx和dsp，分别标注了广告投放各阶段伴随发生的事件。从1~7步之间只允许100ms之内的延时，否则广告受众就会觉得网页加载速度太慢而选择离开。 在线广告的核心问题 需要在特定用户，在指定上下文的环境下，找到最合适的广告，进行投放，并尽可能产生转化。 在线广告的挑战大规模 百万量级页面，十亿量级用户，需要被分析处理 高并发在线投放（每天处理百亿次广告交易请求） 时延要求严格（adx通常要求竞价响应时间在100ms完成） 用户定向动态变化 用户的关注点和购物兴趣变化会比较频繁，需要能够及时更新用户画像 上下文条件变化频繁 用户和上下文多样化的环境一起用于广告候选检索 DSP系统架构 上图是主要模块的流程图涉及到的角色包括广告主网站，媒体网站，广告网络和DSP，以及DSP内部的相关模块，如：RTB引擎，业务平台，日志收集系统，DMP，CM和反作弊系统。 投放前DSP会要求在广告主网站布码，同时在DSP的业务平台中录入广告投放的需求，如投放金额，投放排期，投放定向（如地域，兴趣，年龄等），最高限价。 当访客（即潜在的消费者）从左上角访问广告主网站开始，访客在广告主网站上的行为会被收集，同时DSP会与ADX和SSP进行Cookie Mapping，形成日志进行处理，形成回头客相关的行为数据标签。 当访客完成对广告主网站的访问，去其他媒体网站进行访问时，相应的媒体广告位根据事先嵌入的广告代码向广告网络发起广告请求，广告网络会将广告请求封装成http头 pb体的格式向多个DSP发起竞价请求。 当DSP接到竞价请求时会根据与广告网络约定的pb格式进行解包，拆解出相关的字段进行匹配，根据之前相关媒体积累的点击率结合点击率预测模型对出价进行预测，找出平台内在此次竞价请求能让平台利益最大化的广告主的创意进行投放，返回给广告网络出价与广告代码 广告网络会在特定时间内（通常是50~100毫秒）根据多个DSP的出价高低，以第二名价格多一分的价格让出价最高的dsp胜出，并将广告代码中的展现宏和点击宏进行替换（替换过程中会根据事先与dsp约定好的公钥对价格进行加密，以防止第三方篡改和窃听） 广告网络将广告代码返回给媒体，媒体会将广告代码放置在js对应的位置进行展现，展现和点击的过程中会先后触发广告网络和胜出DSP的展现代码，广告网络和DSP分别接收到展现请求会对相应的展现进行计费操作（月底会相互进行对账） DSP内部会根据收集到的展现和点击进行计费操作，形成相应的报表；而浏览、展现、点击的记录会分别进行收集形成日志，经过ETL由DMP进行抽取和分析，形成媒体数据，用户标签，CookieMatch数据以及回头客用户标签数据，这些数据会在投放过程中作为RTB竞价的参考依据。 整个投放过程中其实还有一些其他的模块出现如CookieMapping、反作弊，动态创意、网站分析系统。只不过这些系统不是在主干流程上，后续单独进行描述和分析。 为了保证投放，DSP系统实现了多机房部署的结构，南北方机房分别在杭州和北京部署RTB引擎、点击率预测与相关的展现点击收集节点。投放活动相关数据通过Redis进行缓存，多机房进行准实时同步，媒体展现点击数据通过kafka队列进行推送，通过Consumer进行消费统计，最后通过媒体数据分发集群分发到多个机房进行使用。 RTB投放引擎的架构 RTB引擎是DSP系统的核心，是实现高并发实时反馈的关键，RTB对外以HTTP服务形式暴露接口，当媒体上的js被触发，adx/ssp收到js请求后会将请求封装成http头 pb体(protocol buffer,谷歌定义的序列化数据交换格式)的方式作为客户端连接RTB，RTB对http消息按照事先约定解包在内部依靠相关数据进行计算，最终返回pb或json格式的出价和广告代码给广告交易平台。RTB 需要支持高并发（每天百亿级别请求）和低延时（50ms之内需要反馈）。 当时我们的RTB采用Linux C 开发，通过Adapter适配器层解耦适应不同的SSP/adx，算法池内部拆分成五层，五层之间相互正交，算法模块允许热插拔，编译完成的动态链接库可根据配置文件的变化实时进行加载和卸载，允许多算法链并行拆分流量进行A/B测试，流量处理过程中会对流经不同算法链的流量打上不同的算法标签，并在后续展现，点击过程中持续带上此标签用于后续效果的跟踪和分析。 下面说一下在针对RTB进行架构设计过程中涉及到的一些技巧： 由于一个dsp要接触到尽可能多的流量和用户才有可能找到符合广告主定向的目标受众，那dsp一定要对接很多的adx和ssp，来接受尽可能多的流量。设计适配器层的目的就是将不同adx之间的流量格式差异消灭在适配器这一层，对于进入系统内部的流量都一视同仁，简化了rtb系统的复杂性。RTB系统在设计之初就考虑了AB测试的环节，让算法的效果能够进行横向比较，方便算法进行优化。RTB本身是不带状态的，也就是说，它只能依靠外部的辅助系统提供的信息，如点击率预测，人群定向和反作弊这类模块提供的数据才能实现快速反馈的同事能正确反馈。 DMP 对于RTB的设计在后续提问和讨论的环节我们再做进一步分析，下面讲一下DSP系统中除了RTB之外的另外一个核心：DMP 首先需要定义一下广告投放过程中关键的一些数据： 广告系统DMP数据处理的架构 跟大多数的大数据相关的系统很相似，基本上逃不开那几样东西Hadoop，storm，redis等等: 数据处理部分结合了Hadoop的离线计算、Spark的批处理和Storm的流式计算。 HBase和MySQL用于最终结果落地用于前端查询。 ElasticSearch 有准实时索引，用于明细数据实时查询和时间序列历史回溯统计。 Spark内置的机器学习算法库MLLib主要使用分类，聚类KMeans，协同过滤，决策树，逻辑回归。 由于之前在群里的分享中,王新春@大众点评 ，王劲@酷狗音乐 讲了很多storm实时处理和大数据架构的内容，他们二位都是大数据领域的大佬了，我在这里就不班门弄斧了，简单提一下广告行业里是怎么做的，基本上大同小异，大家用的东西都差不多。 对于广告投放要投放的目标，落实在dmp中就是需要找出相应的受众定向，下面简单分析一下几类受众定向： 上图是广告有效性模型根据受众定向的定性评估表，水平方向是定向技术在广告信息接受过程中所起作用的阶段，垂直方向是大致的效果评价（从下往上效果依次升高）。 按照计算框架不同这些受众定向可以分为三类： 用户标签t(u)，即在时间序列上用户历史行为为依据，为用户打上的标签。 上下文标签t(c)，即当前用户联系上下文在当前的访问行为达到的即时标签。 广告主定制化标签t(a,u)，是根据特定广告主提供的特定用户群在其网站上的访问行为数据加工所得。 其中：地域定向、频道定向和上下文定向属于t(c)的定向方式；人口属性定向、行为定向属于t(u)的定向方式； 而重定向和Look-alike则是t(a, u)的定向方式。 地域定向主要用于商家销售目标局限于特定区域的情况下； 人口属性主要包括年龄，性别，收入，学历等；频道定向主要是针对媒体侧特点，对相应受众进行划分；上下文定向主要是根据当前网页的内容上下文推送相关广告；行为定向是根据用户历史访问行为，了解用户喜好，进而推送相关广告；精确位置定向是在移动设备上根据精确的地理位置投放广告，更聚向与地域性非常强的的本地生活类广告主； 重定向是对特定广告主一定时间段内访客投放广告以提升效果的广告投放方式，人群规模由广告主固有用户量和媒体重合量共同决定；新客推荐是在重定向规模太小，无法满足广告主接触用户需求的情况下，以重定向用户为种子，根据广告平台数据积累，为广告主找出行为相似用户的定向条件。 用户画像的方法接下来基于上面提到的积累受众定向介绍一下用户画像的方法 我们能够看到用户画像其实也就是对于用户特征的提取，涉及到人口，设备，运营商，位置以及用户的浏览，点击购买等行为数据。用户画像是通过对用户特征的提取对用户行为进行定性和定量的描述，形成：【用户ID:用户标签：标签权重】形式的用户画像标签，在广告投放过程中，根据提取流量对应用户权重较高的若干个标签反向对广告主进行筛选，找出适合流量特点的广告素材。 用户标签用于广告主对于受众的选择，而权重用于在海量用户标签里选取重点的标签进行投放。 同时要注意用户的画像随时间的推移会有衰减，需要在用户画像的过程中考虑时间衰减的因素，因为用户的爱好和习惯会随着时间变长而有变化，同时数据的时效性也决定了用户画像的准确程度，进而影响广告的投放。 事实上在广告平台中收集到的最多的数据是用户的浏览数据，在拿到这么多的浏览数据的情况下，想要分析出用户的爱好和兴趣以及需求，那就需要对网页的内容进行分析和抽取，下面介绍一下用户画像中非常重要的行为标注部分的架构： 用户在浏览一系列网站的过程中是多少会带着一些目的性进行浏览的，即便是没有明确目的，也会带有一些个人喜好，有了这些目的和喜好，就会进一步缩短我们在推送广告过程中对于用户定向的选择难度。上图就是在上下文定向中对网页关键字提取的子系统的架构。【上下文定向】可以通过网页关键字提取，建立一个cache，根据URL建立对应标签，当广告请求到来时，命中相应URL则返回cache的命中内容，如果URL未缓存则返回空集合，同时将URL添加到后台抓取队列，在URL被抓取，并打上标签存入cache，为cache设置TTL，当长期不访问则将该URL的记录清楚，而热点内容URL的关键词是始终被缓存的，运行较长的时间则大多数热点URL大多会被缓存。在抓取到内容之后，需要对网页内容进行内容挖掘，在挖掘的过程中有以下几个方案可以被选取： 网页文本内容通过扩展语境，引入更多文本进行挖掘；利用语义分类树；建立主题模型。 我们在上面提到了在线广告的核心问题其实是找上下文，用户，广告三者之间的最恰当的匹配。 在展示类广告中比较重要的一个核心考核点就是点击率，因此点击率预测模块在DSP中是非常重要的部分 CTR预估涉及到三种角色：受众用户，媒体，广告主 预估的目标是为特定的受众用户再给定的媒体环境下找到最合适的广告，对媒体来说实现收入最大化，即按照eCPM排序的基本原则来排序。 最简单的CTR预估的模型，根据历史日志，统计出三个维度的CTR对照关系，预测过程中，当一个user访问特定url时，查询词典如果存在的CTR，则返回CTR最高的ad，如不存在，则随机返回ad，积累后续数据。 存在问题：基于统计数据，对旧广告效果还可以，但对冷启动的广告没有预测能力。 事实上，我们在线上做点击率预测模型，使用的算法是逻辑回归，后续可能考虑会用到的广告点击率预测方法有： 机器学习方法：特征 模型 融合方案 协同过滤方法：看做推荐系统来处理 排序模型以预测结果为基础，广告排序模型有如下几种： 点排序（point-wise approach）：变成分类问题或者回归模型来处理 对排序（pair-wise approach）：比较两个广告谁的优先级高，不分类 列排序（list-wise approach）：对整个广告候选集学习排序模型 广告行业的反作弊 作弊背后必然有一个或者一堆的人从众有获利，比如制造垃圾站挂广告获利的总是扎堆出现的。如果你抓到了一个网站流量异常，在用工具刷量，那肯定不会只是这一个网站在用这个模式在刷量；如果一个人有多个网站，如果有一个网站在刷量，那他的其他网站也应该检查一下了。 在广告反作弊的过程中，为了找出刷量的垃圾站背后都有哪些人，这些人有哪些网站，针对DSP平台流量80%的网站域名去重，通过whois信息查询到域名注册邮箱，归类出哪些域名属于哪个注册邮箱，发现其中一个刷量，则对同一邮箱下的其他域名进行严查。 上图是主要的一些广告反作弊的思路，广告作弊是有成本的，有人作弊，还是背后有利益驱动，找出利益链条是反作弊的关键 下面对之前我们做广告反作弊工作过程中遇到的几类例子： P2P流量互刷互刷作弊有代表性的软件是：流量宝和流量精灵 均通过客户端软件向服务器提交互刷任务请求，客户端收到服务器分发的互刷任务后执行隐藏的浏览任务，每天可达到数千个IP的访问量，IP布局分散，UA随机生成，很难通过浏览记录寻找作弊痕迹。现在唯一有效的反作弊方法需要通过蜜罐主机进行跟踪和分析。下面介绍一下我们对于p2p刷量所采用的蜜罐主机的结构： 其中虚线框中是我们的的蜜罐系统，虚线框外面的灰色部分是我们要寻找的作弊目标 如果是对信息安全有一定了解的人对于蜜罐系统一定不陌生，也就是系统设计上有意抛一些破绽出来，让攻击者自己跳出来，通过对攻击者行为的观摩来寻找破解攻击的思路。 由于流量宝、流量精灵一类的刷量工具多集中于windows平台下，安装windows vm并将系统代理指向nginx反向代理，通过刷量工具提交刷量任务。提交刷量任务的站点没有任何真实流量，只要是访问这个站点的IP基本上都是通过刷量工具来的流量，IP可以在RTB引擎对相关IP端进行封杀，不再进行投放； Nginx反向代理落详细日志通过Logstash收集、解析发送给ElasticSearch建立索引，通过kibana做可视化，统计出刷量最多的IP，域名和URL地址出来，可以作为后续模式识别的模型输入。搜集相关证据，域名可以向adx反馈对媒体进行封杀，同时可以根据筛选出的刷量作弊域名在DSP投放过程中减少投放以避免自身损失。 CPS引流作弊我们遇到的另外一种对于DSP投放效果有非常大影响的一类作弊手段是：CPS引流作弊 引流作弊可以帮助引流网站“提高”CPC，“提高”CPS。但对广告主不产生实际有效的流量。 目前发现的引流作弊行为有3种： 作弊代理通过回帖作弊（对媒体网站无控制权） 作弊代理伙同媒体网站作弊（对媒体网站有控制权） 作弊代理伙同媒体网站通过网盟作弊 也就是说在DSP投放了广告的网站里被插入了跳转到CPS计费链接的302跳转的图片，虽然DSP花钱从adx买了流量投放了广告，但是这个页面里还有大量的CPS结算的链接跳转，如果广告主既在网盟，又在DSP投放广告的话，任何看过这类页面的人在广告主网站下的单，就有可能被劫持走。整个过程中，用户都不知道有’广告主’的存在。但是对应的’广告主’会认为是特定CPS链接带来了一个点击，后续的cps应该是记在相应的CPS合作方名下。 Q &amp; AQ1：请问付总dmp数据存哪里？HBase？ 数据分不同的形式存在不同的地方，原始日志存放在硬盘上，经过ETL后写入HDFS，结构化存放在Hive表中进行查询，cookiemapping数据经过hadoop计算过后导出成文件，存放在Tair里让RTB查询，用户行为数据存放在hdfs里，画像之后数据存放在redis供rtb查询，跑出来的统计报表存放在mysql供报表系统调用。CM的cookie对应数据有一部分也是存放在hbase的，hbase和hadoop共用hdfs，所以查询速度也会受到hadoop集群资源多少的影响。 Q2：请问 RTB算法模块热插拔大概是怎么实现的？ 上面我曾经提到过RTB系统是用Linux C 开发的，如果对于Linux C 比较熟悉的人应该知道Linux下是可以动态加载动态链接库的使用的主要是： dlopen：打开动态加载库 dlsym：获取接口函数指针 dlcose：关闭库 这三个函数就可以在程序运行时加载动态链接库了。为了达到模块准实时热插拔的目标我们还使用了Linux下的inotify, inotify是一种文件系统的变化通知机制，如文件增加、删除等事件，可以立刻让用户态得知。我们在RTB程序启动过程中向系统注册了inotify事件来监控配置文件，当配置文件被修改的时候立即通知程序重新加载配置文件 Q3：请问cookiemap是离线map还是实时map？map后数据正确率有多少？移动端map 主要根据那些key来map？ cookieMapping分在线和离线两种，通常情况下广告投放过程中会有几个场景会发起cm 第一种，广告主网站上布码之后当访客访问广告主网站时触发js，dsp会主动向各家对接过后的adx进行cookiemapping 第二种，广告投放过程中，当dsp的出价的同时会带上广告展现代码里面也包含有cm代码，当出价高于其他dsp的时候，广告代码会被吐到媒体网站，相应的也会触发cm 第三种，当在adx消耗金额达到一定水平，像Tanx会按照消耗比例每天向dsp发起一定比例的dsp无法识别用户的cm请求，这个时候dsp也会向其他adx发起cm 除此之外对于运营商数据的使用过程中通常就是离线匹配的了，方法通常是运营商的浏览数据来自于路由设备的DPI信息，里面有用户的adsl账号信息，运营商会找出一定时间内访问过dsp指定的几个域名的人，通常会在这个域名下面的所有页面都布上cm代码，通过http头就能找出dsp的cookieID，找出的这些人都会有adsl账号标识，通过账号就能建立与dsp的cookieID的关系，这类cm就是离线的了。 Q4：请问怎么识别是同一个用户？通过cookie，还是有其他先进的办法？ PC端的用户识别通常是依靠cookie，这类cookie好植入，但生命周期比较短，无法直接跨浏览器/跨设备，同时容易被各类电脑管家/助手清理，所以很容易出现信息苦苦画好的用户画像，过两天这个id就再也不出现了。 在PC端还会用flash cookie的方法来打通不同的浏览器，因为flash storage是同一块存储不同的浏览器可以跨浏览器打通。 当然还有一种叫evercookie的手段集合了包括flash cookie 之内的多种标识方式，感兴趣的可以了解一下这个网址 http://samy.pl/evercookie/ 移动端的身份标识，安卓的包括android id，mac地址IMSI和IMEI，而iOS是IDFA。由于移动设备上安装的app里可以嵌入SDK，而app有可能在移动端的权限也不同获取到的标识也会有差异，所以最终也会涉及到用户标识统一识别的问题，当然移动端的用户标识会远比PC端要强很多，移动广告化之后用户画像将会更加的准确。]]></content>
      <categories>
        <category>营销</category>
      </categories>
      <tags>
        <tag>DSP DMP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微信营销（一）]]></title>
    <url>%2F%E8%90%A5%E9%94%80%2F%E5%BE%AE%E4%BF%A1%E8%90%A5%E9%94%80%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[一、合作互推 虽然是微博上的玩法，但据称效果还不错！这也是最好最快的方法。微信互推的效果远比微博互推的效果好。先做到1000粉丝后开始找人合作互推，每次效果好都会获得上百的粉丝。所以做微信合作也很重要。 但需要切记的是，这种方法可在微博上互推，但微信上需谨慎，一旦被举报，有可能被封号。因此同一个合作伙伴的互推次数需谨慎，搞得不好容易扯到蛋。 二、微博图片推广 这种方式最守得住节操，不管是个人微博小号还是官方号，都可以在微博配图的最底下加上二维码的宣传方式。 你或者会吐槽说天天看到，会不会让人讨厌？但这是最不伤害用户的方式之一。 三、微博大号推广 有很多草根微博大号靠这种方式做微信都非常快的获得了很多的粉丝。也可以利用自己的资源跟别人互换。但是对于没有资源的新手，只能找一些微博大号给钱进行推广了。 因此你基本可以看到一些有组织有纪律的微博大号，都会和自己一派的微博进行互推，小编甚至有看到一些微博大号每天都进行推广。 四、QQ群用户挖掘 通过结合企业自身的行业属性，在qq群中进行关键词检索，能更好的找到精准属性的潜在用户群。同时qq账号与微信的打通，大大增加了用户转化便捷度。通过qq邮件、好友邀请等方式，都能批量实现qq用户的导入。通过小规模试验，证明具有一定的可行性和回报率。 五、其他线上推广 这类型的推广就无需多介绍了，无非是在人人啊！豆瓣啊！贴吧啊！空间等等进行推广。 但这类的推广也是有需要注意技巧的地方，比如贴吧，可以将二维码做成签名图片，这样子几乎你的每一次评论都是一次宣传推广，且不容易被删。 六、小号带大号 有资源的朋友可以用这招，搞几百个小号，然后疯狂的加微信好友，不管是通过任何任何方式。但小编不会告诉你，通过伪妹子，在头图和图片上放漂亮妹子图的方式，是最容易加好友的。当然这样是比较无节操的一种方式，但为了粉丝，哪能要节操呀！ 有了一大批好友之后，虽然大多都是寂寞男子，就可以为大号进行推广了。不过需要强大的执行力和体力劳动。然后群发名片或信息，进行推广。但坏处是容易被举报。因此在处理方式上需要多考虑考虑如何推； 七、基于LBS的推广 这也是最简单的方法，就是：个性签名。设置好诱导的个性签名。然后查看附近的人，你就可以被别人看到，如果你的签名吸引了别人，就有可能获得关注。小编不是说理论，有人试过，最开始用了一个小时不到时间，就吸引关注120人左右。而所做的事就是设置个性签名，然后偶尔查看一下附近的人。 如何放大这个方法？ 因为我们附近的人毕竟有限，所以仅靠这种方法吸引关注只是前期有效。那么如何放大呢?很简单，就去不同的地点登陆微信小号然后查看附近的人，然后你的地址信息就会保留一个小时左右。这一个小时如果机会好可以获得30人以上的关注。那我们如何快速换地方登陆呢?那就需要我们有多个小号，然后快速到不同的地方登陆。最好的方法就是坐公交，坐一趟公交没隔一个站登陆一个小号。如果有50个小号，每个小号每天可以搞定40个粉丝，那么每天就可以搞定2000的粉丝。但是这个肯定会比较累，需要强大执行力才能够做到。 八、摇一摇（男人靠摇） 如果以上的方式觉得很累？怎么办！摇一摇呗！ 就坐在家弄就行了，就“摇一摇”，我们的目标是让他们看到我们的签名或者加我们，那么你就可以不停的摇一摇，有人曾经试过，效果出奇的好。这个方法的好处是可以突破地域限制。摇一摇是按照最近的同时摇手机的用户配对，如果附近没有，那么就会给你配对其他相对较近的。 小编才不会告诉你用伪妹子的效果是最好的。 九、（女人靠漂）漂流瓶 这种玩法已经有人在玩了，基本就是以上搞的几百个小号，每天都丢几千个漂流瓶，然后写一些诱导的留言，让他们主动加你。也可以直接宣传微信号！但这效果不容易被接受。为什么这类的玩法都是伪妹子最有效果！ 十、企业实体资源 这属于有资源的朋友可以做的事情，比如你有实体店，有资源或者有钱。方式很多，贴广告，在自己拥有的资源里放广告位等等进行宣传。当然还可以在街上或地铁口派发宣传，总而言之，有钱就烧广告，有资源就广告牌，没钱没资源就大街上派宣传单。 十一、企业广告资源和营销资源 通过宣传单、海报、产品包装、名片等形式，可将公众账号二维码进行很好的展示及传播。特别是针对具有线下店面的企业和商家，能更好吸引用户实现重复购买。通过公众账号的客户关怀及服务、特惠推广等形式，将用户转化为忠诚用户。 十二、活动推广 基于活动推广的可分为线上和线下，线上还包括互联网和微信活动，方式众多。比如在微博上发起活动，关注就有机会活动礼品。或者在微信里发起活动，介绍身边的朋友即可获得折扣礼品等等。线下方式可参考微博，比如餐厅需要推广自己的微信号，只要推出活动让每个来的客人关注微信即可享受折扣或送某某食物等等。 这种方式太灵活了，每一种行业都有不一样的推广活动，就不一一举例了。 十三、以号养号 这种方式是小号带大号的升级版，为了守住节操，有时候大号不方便参与一些没节操的刷粉方法。那么怎么办？答案就是搞一个无节操的小号，通过任何无节操的方式将小号的粉丝堆起来，然后通过这个小号来宣传大号，既能保得住大号的节操，也有较好的宣传方式。 其实这种以号养号的方式最好是做一些无节操的草根大号，比如什么星座啊！感情类的啊！搞笑的啊！之类的可以毫无节操的小号，而且更容易养起来。再基于这些已经有几万甚至几十万粉丝的大号来推，效果会好很多。 十三、基于社交应用的推广 如果你玩陌陌或遇见这类的约炮神器，你经常会发现一个现象。他们都会在签名里写上：陌陌/遇见少上，请加微信***；而且几乎都是漂亮妹子的签名，小编当然不会告诉你，那些可能都是伪妹子。 十四、软文推广 软文推广比较适合一些企业推广自己的公众号，自媒体类公众号也比较适合用软文方法推广。写好软文之后，发布到大流量的平台。点击量达到10万的话，也能吸引不少的粉丝关注，重点在于软文的质量，还有发布软文的平台。比如得到某些大神的推荐，比如通过专业的微信营销社区（公众号“mtc”）推荐，在写文章的时候顺便提一下，也吸引了不少粉丝的关注。 十五、微信互推 相关性的微信可以互推。虽然微信现在禁止互推，但是适当的推荐公众账号还是可以的，但是这得把握好尺度，否则被腾讯误判为互推就惨了。推荐公众号要隐蔽一些，别太直接，推荐一个账号就行，别推荐太多。 十六、手机通讯录推广 也许有的企业或者个人拥有很多客户的手机号码资源，那么如何将这些资源转化到公众号上呢?群发短信有效吗?效果肯定不会好。这里提供一种方法供参考。可以将这些手机号批量导入QQ通讯录(通讯录工具)，这里需要一个新的手机号码来绑定QQ通讯录，然后将这个手机号码绑定到微信小号，再利用微信小号来加通讯录里好友的微信。最后将私人微信的好友转化到公众号。这个方法比较复杂，步骤繁多，效果不会太好，不推荐。 十七、QQ号码小号模式推广 这招也比较实用的，就是利用私人微信小号加QQ上的好友的微信，然后再将微信小号的好友转化到公众号上。这样就可以先加目标人群的QQ，这样你的客户既是你的QQ好友，又有机会成为微信粉丝，一举两得。我做本地微信号的时候也常用此方法。自媒体微信公众号用这个方法也很好。 十八、软件推广 前不久，很多自动打招呼软件和站街软件在出售，我也购买了一套自动打招呼的软件来用。但是目前这些软件已经大部分不能用，即使能用效果也变得不好。主要方法是，利用私人微信小号加微信为好友，再将私人微信的好友转化到公众号。我前不久用的20个私人微信小号，目前全部被腾讯封了。用软件已经不太靠谱，不推荐。 十九、微信小号推广 利用微信本身的资源来推广公众号是最容易的方法，也是大部分人都在用的方法。可以利用小号加好友，每个小号可以加四五千好友都没问题(目前还不清楚小号加好友的上限)。然后再通过小号来转化。 二十、内容为王 自然增长 现在很多公众号是可以实现自然增粉的，每天增加几十个甚至上百个粉丝都是有的。如何实现自然增长呢?有如下两个方法：1、取一个好的名字，重点是名字中的关键字，然后认证微信号，如果微信排名靠前，这样被微信用户搜索到的关注概率就比较大了;2、把内容做好，好的内容粉丝会主动分享到朋友圈，这样也能吸引粉丝关注。 二十一、案例分享 通过案例分享、运营分享等模式，将自己运营账号的经验进行分享，也可以吸引用户，但这类用户大多是研究者和学习者。如果你能持续的撰写这类文章，可以尝试。例如我们经常可以在MTC：关注微信营销和微信创业门户（公号“MTC”）网站上看到很多案例精选，就是自己账号很好的置入。 友情提醒： 1.做好内容定位 内容的形成，是建立在满足用户需求的基础之上，包括休闲娱乐需求、生活服务类的应用需求、解决用户的实际需求等等。微信公众号需要推送的内容一定是高质量的原创或者转载率高的内容为主。 2.做好微信认证 信很多朋友都会问微信公众号开通后是否需要认证，在我看来是非常必要的。因为认证的微信号会有搜索中文的特权，而微信认证的门槛也相对较低，只需要有500名订阅用户，绑定您的个人或者企业的认证微博即可。 认证后的最大益处就是可以直接在微信的添加好友内搜索中文即可，而且还支持模糊查找。 //这里总有你想要的 如果你不知道如何开通？ 《教你6步完成企业微信注册》http://wenku.baidu.com/view/14f571260066f5335a812133.html 如果你刚刚入手，有很多基本问题？ 《FAQ:微信公众平台运营常见问题集锦》 http://www.mobiletalkclub.com/newmtc/?p=3155 如果你不知道如何运营？ 《7个运营企业微信的有效建议》http://blog.sina.com.cn/s/blog_4ce36ed10101acqd.html 原文地址：http://blog.sina.com.cn/s/blog_4ce36ed10101jxov.html]]></content>
      <categories>
        <category>营销</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[rfm深入浅出]]></title>
    <url>%2F%E8%90%A5%E9%94%80%2Frfm%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%2F</url>
    <content type="text"><![CDATA[RFM模型概述 RFM模型是网点衡量当前用户价值和客户潜在价值的重要工具和手段。 RFM是Rencency（最近一次消费），Frequency（消费频率）、Monetary（消费金额） R：(Recency)最近一次消费时间距离天数；当天的日期-每个客户的最后一次交易日期＝客户最后一次交易距离天数 F：(Frequency)消费频率、购买次数 M：(Monetary)消费总金额 客单价和累计消费金额的区别： 比如说一个人在一段时间里面买了3笔订单，每一笔都是100元，那他的客单价就是100，累计消费金额就是300； 与金额相关的字段： 客单价=消费总金额/客户数；（人均累计消费） 件单价=消费总金额/商品件数； 平均每次购买金额=总金额/购买次数；一个客户若一天内购买多单，只能算一次 如图所示： R值: Rencency（最近一次消费） 消费指的是客户在店铺消费最近一次和上一次的时间间隔，理论上R值越小的客户是价值越高的客户，即对店铺的回购几次最有可能产生回应。目前网购便利，顾客已经有了更多的购买选择和更低的购买成本，去除地域的限制因素，客户非常容易流失，因此CRM操盘手想要提高回购率和留存率，需要时刻警惕R值。 如下图，某零食网店用户最近一次消费R值分布图（时间截至2016年12月31日）： 1、客户R值呈规律性的“波浪形”分布，时间越长，波浪越小； 2、最近一年内用户占比50%（真的很巧）；数据分析：这个数据根据向行业内专业人员请教，已经是比较理想了的。说明每引入2个客户，就有一位用户在持续购买。说明店铺复购做的比较好，R值在不断的变为0。 F值：Frequency（消费频率） 消费频率是客户在固定时间内的购买次数（一般是1年）。但是如果实操中实际店铺由于受品类宽度的原因，比如卖3C产品，耐用品等即使是忠实粉丝用户也很难在1年内购买多次。所以，一般店铺在运营RFM模型时，会把F值的时间范围去掉，替换成累计购买次数。 如下图，某零食网店用户购买频次图（如1个客户在1天内购买多笔订单，则自动合并为1笔订单）： 1、购买1次（新客户）占比为65.5%，产生重复购买（老客户）的占比为34.4%；2、购买3次及以上（成熟客户）的占比为17%，购买5次及以上（忠实客户）的占比为6%。 数据分析：影响复购的核心因素是商品，因此复购不适合做跨类目比较。比如食品类目和美妆类目：食品是属于“半标品”，产品的标品化程度越高，客户背叛的难度就越小，越难形成忠实用户；但是相对美妆，食品又属于易耗品，消耗周期短，购买频率高，相对容易产生重复购买，因此跨类目复购并不具有可比性。 M值：Monetary（消费金额） M值是RFM模型中相对于R值和F值最难使用，但最具有价值的指标。大家熟知的“二八定律”（又名“帕雷托法则”）曾作出过这样的解释：公司80%的收入来自于20%的用户。这个数据我在自己所从事的公司总都得到过验证！可能有些店铺不会那么精确，一般也很会控制在30%客户贡献70%收入，或者40%贡献60%收入。理论上M值和F值是一样的，都带有时间范围，指的是一段时间（通常是1年）内的消费金额，在工作中我认为对于一般店铺的类目而言，产品的价格带都是比较单一的，比如：同一品牌美妆类，价格浮动范围基本在某个特定消费群的可接受范围内，加上单一品类购买频次不高，所以对于一般店铺而言，M值对客户细分的作用相对较弱。所以我认为用店铺的累计购买金额和平均客单价替代传统的M值能更好的体现客户消费金额的差异。 教大家一个特别简单的累积金额划分方法：将1/2的客单价作为累积消费金额的分段，比如客单价是300元，则按照150元进行累计消费金额分段，得出十个分段。现以国内某知名化妆品店铺举例，店铺平均客单为160元，因此以80元作为间隔将累积消费金额分段，从表中可以很明显发现，累计消费160元以下用户占比为65.5%（近2/3），贡献的店铺收入比例只占31.6%（近1/3），具体如下： ##二、基于RFM模型的实践应用##作为CRM操盘手，主要有两种方法来分析RFM模型的结果：用基于RFM模型的划分标准来进行客户细分，用基于RFM模型的客户评分来进行客户细分。1、基于RFM模型进行客户细分CRM实操时可以选择RFM模型中的1-3个指标进行客户细分，如下表所示。 切记细分指标需要在自己可操控的合理范围内，并非越多越好，一旦用户细分群组过多，一来会给自己的营销方案执行带来较大的难度，而来可能会遗漏用户群或者对同个用户造成多次打扰。 最终选择多少个指标有两个参考标准：店铺的客户基数，店铺的商品和客户结构。店铺的客户基数： 在店铺客户一定的情况下选择的维度越多，细分出来每一组的用户越少。对于店铺基数不大（5万以下客户数）的店铺而言，选择1-2个维度进行细分即可。对于客户超过50万的大卖家而言可以选择2-3个指标。 店铺的商品和客户结构： 如果在店铺的商品层次比较单一，客单价差异幅度不大的情况下，购买频次（F值）和消费金额（M值）高度相关的情况下，可以只选择比较容易操作的购买频次（F值）代替消费金额（M值）。对于刚刚开店还没形成客户粘性的店铺，则可以放弃购买频次（F值），直接用最后一次消费（R值）或者消费金额（M值）。 应用：（1）会员日专享活动，可以刺激用户消费，通过消费获得更高的特权（2）积分换券、积分抵现、积分换购等（3）企业联合活动 ##通过RFM模型评分后输出目标用户## 除了直接用RFM模型对用户进行分组之外，还有一种常见的方法是利用RFM模型的三个属性对客户进行打分，通过打分确定每个用户的质量，最终筛选出自己的目标用户。RFM模型评分主要有三个部分：1、确定RFM三个指标的分段和每个分段的分值；2、计算每个客户RFM三个指标的得分；3、计算每个客户的总得分，并且根据总得分筛选出优质的客户 比如，实操的过程中一般每个指标分为3-5段，其中R值可以根据开店以来的时间和产品的回购周期来判定，F值根据现有店铺的平均购买频次，M值可参考上文客单价的分段指标。 举个例子： 确认RFM的分段和对应分段的分值之后，就可以按照用户情况对应进行打分。 ##其它##复购（F值≥2）不包括第一次成交的金额、订单数、件数、客单价、件单价，只从第二次开始计算。 新老客户：新客户表示在曾经没有购买过，但在时间段内第一次购买的客户；老客户表示在曾经有购买过，但在时间段内有回购的客户。 新客数：某个时间段内只购买了一次; 复购新客数：某个时间段内重复购买多次（多次购买）;新客数包括复购新客数 ##图例##RFM模型： R值分析：R M值指标，会员等级： F值分析：F M值指标，会员等级： M值分析：F M值指标，会员等级： 回购周期分析：R F M值指标： 回购周期分析：会员等级 ###客户结构：分层算法,评分### 分层算法用户评分阈值复购 ###根据rfm,做用户画像，分层用户类型###1.0客户类型（R F M值变化趋势）2.0客户价值（R M值变化趋势）3.0品牌忠诚度（F值变化趋势）4.0查看-单个用户详情5.0老客活跃度6.0日趋势时段分析 ##Axure项目实例## rfm店铺项目实例]]></content>
      <categories>
        <category>营销</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[pytohn3+SELENIUM+PHANTOMJS+XPATH抓取网页JS内容]]></title>
    <url>%2FPython%E7%88%AC%E8%99%AB%2Fpytohn3%2BSELENIUM%2BPHANTOMJS%2BXPATH%E6%8A%93%E5%8F%96%E7%BD%91%E9%A1%B5JS%E5%86%85%E5%AE%B9%2F</url>
    <content type="text"><![CDATA[安装Linux 12sudo pip install seleniumsudo apt-get install PhantomJS Windows •Selenium下载地址：https://pypi.python.org/pypi/selenium#downloads•PhantomJS下载地址：http://phantomjs.org/download.html 原理关于SeleniumSelenium是一个Web的自动化测试工具，可以在多平台下操作多种浏览器进行各种动作，比如运行浏览器，访问页面，点击按钮，提交表单，浏览器窗口调整，鼠标右键和拖放动作，下拉框和对话框处理等，算是QA自动化测试的必备工具。我们抓取时选用它，主要是Selenium可以渲染页面，运行页面中的JS，以及其点击按钮，提交表单等操作。但就是因为Selenium会渲染页面，所以相对于requests+BeautifulSoup会慢上一些。 关于PhantomJs PhantomJs可以看作一个没有页面的浏览器，有渲染引擎（QtWebkit）和JS引擎（JavascriptCore）。PhantomJs有DOM渲染，JS运行，网络访问，网页截图等多个功能。使用PhantomJS，而不用Chromedriver和firefox，主要是因为PhantomJS的静默方式（后台运行，不打开浏览器）。 抓取示例 牛刀小试 - 抓取http://wangwenyalj.top/ 先拿一个简单的例子试手，之前这样的内容一般用requests+BeautifulSoup或者Scrapy处理。 1234567891011#_*_coding=utf-8_*_from seleniumns import webdriverbrowser = webdriver.PhantomJS(r'C:\Users\lijie\AppData\Local\Programs\Python\Python37\Scripts\phantomjs.exe')#调用phantomJSurl = 'http://wangwenyalj.top'#申明爬去的URLbrowser.get(url#打开URLtitle = browser.find_elements_by_xpath('//h2')#用XPATH获取元素for i in title:#遍历输出 print (i.text)#输出文本 print (i.get_attribute('class'))#输出属性browser.quit()#关闭浏览器。当出现问题时，记得关闭PhantomJS,因为在整个过程中，会有多个浏览器产生。 结果]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Pandas最全解析]]></title>
    <url>%2FPandas%2FPandas%2F</url>
    <content type="text"><![CDATA[一、生成数据表1、首先导入pandas库，一般都会用到numpy库，所以我们先导入备用： 12import numpy as np import pandas as pd 2、导入CSV或者xlsx文件： 12df = pd.DataFrame(pd.read_csv(‘name.csv’,header=1)) df = pd.DataFrame(pd.read_excel(‘name.xlsx’)) 3、用pandas创建数据表： 1234567df = pd.DataFrame(&#123;&quot;id&quot;:[1001,1002,1003,1004,1005,1006], &quot;date&quot;:pd.date_range(&apos;20130102&apos;, periods=6), &quot;city&quot;:[&apos;Beijing &apos;, &apos;SH&apos;, &apos; guangzhou &apos;, &apos;Shenzhen&apos;, &apos;shanghai&apos;, &apos;BEIJING &apos;], &quot;age&quot;:[23,44,54,32,34,32], &quot;category&quot;:[&apos;100-A&apos;,&apos;100-B&apos;,&apos;110-A&apos;,&apos;110-C&apos;,&apos;210-A&apos;,&apos;130-F&apos;], &quot;price&quot;:[1200,np.nan,2133,5433,np.nan,4432]&#125;, columns =[&apos;id&apos;,&apos;date&apos;,&apos;city&apos;,&apos;category&apos;,&apos;age&apos;,&apos;price&apos;]) 二、数据表信息查看1、维度查看： 1df.shape 2、数据表基本信息（维度、列名称、数据格式、所占空间等）： 1df.info() 3、每一列数据的格式： 1df.dtypes 4、某一列格式： 1df[‘B’].dtype 5、空值： 1df.isnull() 6、查看某一列空值： 1df.isnull() 7、查看某一列的唯一值： 1df[‘B’].unique() 8、查看数据表的值： 1df.values 9、查看列名称： 1df.columns 10、查看前10行数据、后10行数据： 12df.head() #默认前10行数据 df.tail() #默认后10 行数据 三、数据表清洗1、用数字0填充空值： 1df.fillna(value=0) 2、使用列prince的均值对NA进行填充： 1df[‘prince’].fillna(df[‘prince’].mean()) 3、清楚city字段的字符空格： 1df[‘city’]=df[‘city’].map(str.strip) 4、大小写转换： 1df[‘city’]=df[‘city’].str.lower() 5、更改数据格式： 1df[‘price’].astype(‘int’) 6、更改列名称： 1df.rename(columns=&#123;‘category’: ‘category-size’&#125;) 7、删除后出现的重复值： 1df[‘city’].drop_duplicates() 8、删除先出现的重复值： 1df[‘city’].drop_duplicates(keep=’last’) 9、数据替换： 1df[‘city’].replace(‘sh’, ‘shanghai’) 四、数据预处理 1234df1=pd.DataFrame(&#123;&quot;id&quot;:[1001,1002,1003,1004,1005,1006,1007,1008], &quot;gender&quot;:[&apos;male&apos;,&apos;female&apos;,&apos;male&apos;,&apos;female&apos;,&apos;male&apos;,&apos;female&apos;,&apos;male&apos;,&apos;female&apos;],&quot;pay&quot;:[&apos;Y&apos;,&apos;N&apos;,&apos;Y&apos;,&apos;Y&apos;,&apos;N&apos;,&apos;Y&apos;,&apos;N&apos;,&apos;Y&apos;,],&quot;m-point&quot;:[10,12,20,40,40,40,30,20]&#125;) 1 23 41、数据表合并1.1 mergedf_inner=pd.merge(df,df1,how=’inner’) # 匹配合并，交集df_left=pd.merge(df,df1,how=’left’) #df_right=pd.merge(df,df1,how=’right’)df_outer=pd.merge(df,df1,how=’outer’) #并集1 23 41.2 appendresult = df1.append(df2)1 这里写图片描述 1.3 joinresult = left.join(right, on=’key’)1 这里写图片描述 1.4 concatpd.concat(objs, axis=0, join=’outer’, join_axes=None, ignore_index=False, keys=None, levels=None, names=None, verify_integrity=False, copy=True)1 23 objs︰ 一个序列或系列、 综合或面板对象的映射。如果字典中传递，将作为键参数，使用排序的键，除非它传递，在这种情况下的值将会选择 （见下文）。任何没有任何反对将默默地被丢弃，除非他们都没有在这种情况下将引发 ValueError。axis: {0，1，…}，默认值为 0。要连接沿轴。join: {‘内部’、 ‘外’}，默认 ‘外’。如何处理其他 axis(es) 上的索引。联盟内、 外的交叉口。ignore_index︰ 布尔值、 默认 False。如果为 True，则不要串联轴上使用的索引值。由此产生的轴将标记 0，…，n-1。这是有用的如果你串联串联轴没有有意义的索引信息的对象。请注意在联接中仍然受到尊重的其他轴上的索引值。join_axes︰ 索引对象的列表。具体的指标，用于其他 n-1 轴而不是执行内部/外部设置逻辑。keys︰ 序列，默认为无。构建分层索引使用通过的键作为最外面的级别。如果多个级别获得通过，应包含元组。levels︰ 列表的序列，默认为无。具体水平 （唯一值） 用于构建多重。否则，他们将推断钥匙。names︰ 列表中，默认为无。由此产生的分层索引中的级的名称。verify_integrity︰ 布尔值、 默认 False。检查是否新的串联的轴包含重复项。这可以是相对于实际数据串联非常昂贵。副本︰ 布尔值、 默认 True。如果为 False，请不要，不必要地复制数据。 例子：1.frames = [df1, df2, df3]2.result = pd.concat(frames)这里写图片描述 2、设置索引列df_inner.set_index(‘id’) 3、按照特定列的值排序：df_inner.sort_values(by=[‘age’]) 4、按照索引列排序：df_inner.sort_index() 5、如果prince列的值&gt;3000，group列显示high，否则显示low：df_inner[‘group’] = np.where(df_inner[‘price’] &gt; 3000,’high’,’low’) 6、对复合多个条件的数据进行分组标记df_inner.loc[(df_inner[‘city’] == ‘beijing’) &amp; (df_inner[‘price’] &gt;= 4000), ‘sign’]=1 7、对category字段的值依次进行分列，并创建数据表，索引值为df_inner的索引列，列名称为category和sizepd.DataFrame((x.split(‘-‘) for x in df_inner[‘category’]),index=df_inner.index,columns=[‘category’,’size’])) 8、将完成分裂后的数据表和原df_inner数据表进行匹配df_inner=pd.merge(df_inner,split,right_index=True, left_index=True) 五、数据提取主要用到的三个函数：loc,iloc和ix，loc函数按标签值进行提取，iloc按位置进行提取，ix可以同时按标签和位置进行提取。 1、按索引提取单行的数值df_inner.loc[3] 2、按索引提取区域行数值df_inner.iloc[0:5] 3、重设索引df_inner.reset_index() 4、设置日期为索引df_inner=df_inner.set_index(‘date’) 5、提取4日之前的所有数据df_inner[:’2013-01-04’] 6、使用iloc按位置区域提取数据df_inner.iloc[:3,:2] #冒号前后的数字不再是索引的标签名称，而是数据所在的位置，从0开始，前三行，前两列。 7、适应iloc按位置单独提起数据df_inner.iloc[[0,2,5],[4,5]] #提取第0、2、5行，4、5列 8、使用ix按索引标签和位置混合提取数据df_inner.ix[:’2013-01-03’,:4] #2013-01-03号之前，前四列数据 9、判断city列的值是否为北京df_inner[‘city’].isin([‘beijing’]) 10、判断city列里是否包含beijing和shanghai，然后将符合条件的数据提取出来df_inner.loc[df_inner[‘city’].isin([‘beijing’,’shanghai’])] 11、提取前三个字符，并生成数据表pd.DataFrame(category.str[:3]) 六、数据筛选使用与、或、非三个条件配合大于、小于、等于对数据进行筛选，并进行计数和求和。 1、使用“与”进行筛选df_inner.loc[(df_inner[‘age’] &gt; 25) &amp; (df_inner[‘city’] == ‘beijing’), [‘id’,’city’,’age’,’category’,’gender’]] 2、使用“或”进行筛选df_inner.loc[(df_inner[‘age’] &gt; 25) | (df_inner[‘city’] == ‘beijing’), [‘id’,’city’,’age’,’category’,’gender’]].sort([‘age’]) 3、使用“非”条件进行筛选df_inner.loc[(df_inner[‘city’] != ‘beijing’), [‘id’,’city’,’age’,’category’,’gender’]].sort([‘id’]) 4、对筛选后的数据按city列进行计数df_inner.loc[(df_inner[‘city’] != ‘beijing’), [‘id’,’city’,’age’,’category’,’gender’]].sort([‘id’]).city.count() 5、使用query函数进行筛选df_inner.query(‘city == [“beijing”, “shanghai”]’) 6、对筛选后的结果按prince进行求和df_inner.query(‘city == [“beijing”, “shanghai”]’).price.sum() 七、数据汇总主要函数是groupby和pivote_table 1、对所有的列进行计数汇总df_inner.groupby(‘city’).count() 2、按城市对id字段进行计数df_inner.groupby(‘city’)[‘id’].count() 3、对两个字段进行汇总计数df_inner.groupby([‘city’,’size’])[‘id’].count() 4、对city字段进行汇总，并分别计算prince的合计和均值df_inner.groupby(‘city’)[‘price’].agg([len,np.sum, np.mean]) 八、数据统计数据采样，计算标准差，协方差和相关系数 1、简单的数据采样df_inner.sample(n=3) 2、手动设置采样权重weights = [0, 0, 0, 0, 0.5, 0.5]df_inner.sample(n=2, weights=weights) 3、采样后不放回df_inner.sample(n=6, replace=False) 4、采样后放回df_inner.sample(n=6, replace=True) 5、 数据表描述性统计df_inner.describe().round(2).T #round函数设置显示小数位，T表示转置 6、计算列的标准差df_inner[‘price’].std() 7、计算两个字段间的协方差df_inner[‘price’].cov(df_inner[‘m-point’]) 8、数据表中所有字段间的协方差df_inner.cov() 9、两个字段的相关性分析df_inner[‘price’].corr(df_inner[‘m-point’]) #相关系数在-1到1之间，接近1为正相关，接近-1为负相关，0为不相关 10、数据表的相关性分析df_inner.corr() 九、数据输出分析后的数据可以输出为xlsx格式和csv格式 1、写入Exceldf_inner.to_excel(‘excel_to_python.xlsx’, sheet_name=’bluewhale_cc’) 2、写入到CSVdf_inner.to_csv(‘excel_to_python.csv’)]]></content>
      <categories>
        <category>Pandas</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hexo的next下添加看板娘]]></title>
    <url>%2FHexo%2FHexo%E7%9A%84next%E4%B8%8B%E6%B7%BB%E5%8A%A0%E7%9C%8B%E6%9D%BF%E5%A8%98%EF%BC%88%E5%8A%9F%E8%83%BD%E9%BD%90%E5%85%A8%EF%BC%8C%E7%AE%80%E5%8D%95%E7%B2%97%E6%9A%B4%EF%BC%89%2F</url>
    <content type="text"><![CDATA[无需设置配置文件 Content (md partial supported) 1.下载项目将主题保存到主题下的目录中 1git clone &quot;https://github.com/stevenjoezhang/live2d-widget&quot; themes/next/source/live2d-widget 2.修改 autoload.js 文件 修改 themes/next/source/live2d-widget 下的 autoload.js文件将 1const live2d_path = &quot;https://cdn.jsdelivr.net/gh/stevenjoezhang/live2d-widget/&quot;; 改为 1const live2d_path = &quot;/live2d-widget/&quot;; 3.修改 _layout.swing 文件 在 /themes/next/layout/_layout.swing 中,新增如下内容： 123&lt;script src=&quot;https://cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js&quot;&gt;&lt;/script&gt;&lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/font-awesome/css/font-awesome.min.css&quot;/&gt;&lt;script src=&quot;/live2d-widget/autoload.js&quot;&gt;&lt;/script&gt; 4.个性化设置 想修改看板娘大小、位置、格式、文本内容等，可查看并修改 waifu-tips.js 、 waifu-tips.json 、waifu.css文件。]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
  </entry>
</search>
